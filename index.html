<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="MUIRBENCH: A Comprehensive Benchmark for
Robust Multi-image Understanding">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Benchmark, Multimodal Learning, Vision and Language Dataset, Vision Language Model, LLM, VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MUIRBench</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zeyofu.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zeyofu.github.io/blink/">
            BLINK
          </a>
          <a class="navbar-item" href="https://visualsketchpad.github.io/">
            Visual Sketchpad
          </a>
          <a class="navbar-item" href="https://zeyofu.github.io/CommonsenseT2I/">
            Commonsense-T2I
          </a>
        </div>
      </div>
    </div>

  </div>
  </nav>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF (NeurIPS 2023)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><img src="static/images/icon.png" width="50" />MUIRBench: A Comprehensive Benchmark for Robust Multi-image Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://feiwang96.github.io/" target="_blank"><font color="#B082C9"><b>Fei Wang</b></font></a><sup>1*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://zeyofu.github.io/" target="_blank"><font color="#B082C9"><b>Xingyu Fu</b></font></a><sup>2*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.cis.upenn.edu/~danroth/" target="_blank"><font color="#B082C9"><b>Dan Roth</b></font></a><sup>2</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://muhaochen.github.io/" target="_blank"><font color="#B082C9"><b>Muhao Chen</b></font></a><sup>4</sup>&emsp;
                </span>
                <!-- <span class="author-block">
                  <a href="http://www.ranjaykrishna.com/" target="_blank"><font color="#B082C9"><b>Ranjay Krishna</b></font></a><sup>2,3†</sup>&emsp; -->
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>USC&emsp;
                      <sup>2</sup>UPenn&emsp;
                      <sup>4</sup>UC Davis&emsp;
                      <!-- <sup>6</sup>Cornell University&emsp; -->
                      <sup>*</sup>Equal Leadership&emsp;
                      <sup>†</sup>Equal Contribution
                    </span>
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                  </div>

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.09411.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>HF Dataset</span>
                    </a>
                  </span>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/zeyofu/BLINK_Benchmark" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                <span class="link-block">
                  <a href="https://twitter.com/XingyuFu2/status/1781368539213082683" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
                </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">What is MUIRBench?</h2>
      <h2 class="subtitle has-text-justified">
        <span style="font-weight:bold;">MUIRBench  </span>
        is a benchmark containing 11,264 images and 2,600 multiple-choice questions, providing robust evaluation on 12 multi-image understanding tasks. </h2>
      <img src="static/images/teaser.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      <h2 class="hero-body has-text-centered">
        <!-- <br> -->
        Each example comes from one task in <span style="font-weight:bold;">MUIRBench</span>, presenting diverse multi-image relations.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- MUIRBench Comparison -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">MUIRBench -- Novel Features</h2>
        <!-- <h2 class="title is-3">MUIRBench Benchmark -- Unique Features of MUIRBench?</h2> -->
        <h2 class="content has-text-justified">
        
        <ul>
          <li><b>MUIRBench</b> incorporates <i><b>diverse visual prompts</b></i>, like circles, boxes, and image masks, while previous benchmarks only have text questions and answers.</li>
          <li><b>MUIRBench</b> evaluates a more comprehensive range of <i><b>visual perception abilities</b></i>, like multi-view reasoning, depth estimation, and reflectance estimation. Prior benchmarks are generally more focused on recognition-based VQA. </li>
          <li><b>MUIRBench</b> contains <i><b>"visual" commonsense</b></i> problems that humans can answer within seconds, while prior benchmarks like MMMU require domain knowledge</li>
        </ul>
        </h2>
        <img src="static/images/comparison.png" height="100%"/>
        <h2 class="content has-text-centered">
          <b>MUIRBench</b> has several novel features different from previous benchmarks.
        </h2>
        <h2 class="content has-text-justified">
        
          <ul>
            <li><b>MUIRBench</b> covers 14 perception-demanding tasks, inspired by classical computer vision problems. While these problems only takes human a "MUIRBench" to solve, they exceed the capabilities of current multimodal large language models.</li>
          </ul>
          </h2>
        <div class="myrow">
          <div class="mycolumn">
            <img src="static/images/dataset-circular-bar.png" style="width:90%">
          </div>
          <div class="mycolumn">
            <img src="static/images/radar_v1.png" style="width:100%">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <b>MUIRBench</b>, a new benchmark for multimodal language models (LLMs) that focuses on core visual perception abilities not found in other evaluations. Most of the <b>MUIRBench</b> tasks can be solved by humans “within a MUIRBench” (e.g., <i>relative depth estimation, visual correspondence, forensics detection, and multi-view reasoning</i>). However, we find these perception-demanding tasks cast significant challenges for current multimodal LLMs because they resist mediation through natural language. <b>MUIRBench</b> reformats 14 classic computer vision tasks into 3,807 multiple-choice questions, paired with single or multiple images and visual prompting. While humans get 95.70% accuracy on average, <b>MUIRBench</b> is surprisingly challenging for existing multimodal LLMs: even the best-performing GPT-4V and Gemini achieve accuracies of 51.26% and 45.72%, only 13.17% and 7.63% higher than random guessing, indicating that such perception abilities have not “emerged” yet in recent multimodal LLMs. Our analysis also highlights that specialist CV models could solve these problems much better, suggesting potential pathways for future improvements. We believe <b>MUIRBench</b> will stimulate the community to help multimodal LLMs catch up with human-level visual perception.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Results</h2>
        <img src="static/images/qual.png" width="100%"/>
        <!-- <h2 class="content has-text-centered">
          Qualitative results on <b>MUIRBench</b>. 
        </h2> -->
        <h2 class="content has-text-justified">
          For each task, we show the choice of LLaVAv1.6-34B, Qwen-VL-Max, Gemini Pro, GPT-4V, and humans. Red choice indicates the ground truth. Notice that the markers are intentionally enlarged for visualization purposes, and we make some images inset images to save space. For IQ test, the third image is constructed by overlaying the first and second images.
        </h2>
        
      </div>
    </div>
  </div>
</section>


<!-- Paper Quantitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Quantitative Results</h2>
        <img src="static/images/quan.png" height="90%"/>
        <h2 class="content has-text-centered">
          Results of different models on the <b>MUIRBench</b> test set. The first row shows
          task names and number of test data.
        </h2>
        <h2 class="content has-text-justified">
          The mean accuracy of 7B and 13B open-source Multimodal LLMs hover around <b>35–42%</b>, which is similar to <b>random guess (38.09%)</b>. The most proficient open-source model, LLaVA-v1.6-34B, achieves an accuracy of 45.05%. Even the most advanced models, GPT-4V and Gemini Pro and Claude 3 OPUS, achieve accuracies of only 51.26%, 45.72%, and 44.11% respectively. Their performance are merely 13.17%, 7.63% and 6.02% better than random guessing and lag behind human performance by 44.44%, 49.98% and 51.59%. Notably, for certain tasks such as jigsaw, semantic correspondence, multi-view reasoning, object localization, and relative reflectance, some multimodal LLMs even underperform compared to random guessing.
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper Analysis -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Experiment Analysis</h2> <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">
        1. Is dense captioning all you need for a multimodal LLM benchmark?
        <h2 class="content has-text-justified">
          To answer the question, we convert images into task-agnostic dense image captions with GPT-4V, and follow with a text-based LLM. The dense caption describes detailed information about the image and the visual prompts (e.g., where each circle is), using language. We experiment with <b>MUIRBench, MMBench</b> and <b>MMMU</b>. Surprisingly, we find that the Caption + LLM setting achieves better results on MMBench and MMMU then MUIRBench. These results indicate that image captions carry a large portion of visual information needed to answer other benchmarks. Meanwhile, <b>MUIRBench requires advanced perceptual abilities beyond what is currently attainable with general captions</b>.
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/caption_results.png" width="60%"/> </div>
        <h2 class="content has-text-centered">
          Caption+LLM achieves good results on MMBench and MMMU, but failes on <b>MUIRBench</b>.
        </h2>
        <br>
        <h2 class="title is-5">2. Visual prompting can effect a lot on multimodal LLMs.</h2>
        <h2 class="content has-text-justified">
          We analyze the effect of circle sizes and colors on multiple tasks in MUIRBench. The experiments suggest that visual prompting can have a big impact on multimodal LLM performance, and improving visual prompts or improving model robustness to prompt variation is a promising direction for future research.
        </h2>
        <img src="static/images/visual_prompt.png" height="100%"/>
        <h2 class="content has-text-justified">
          We find that the optimal circle size is task-dependent and on average 10px circles work the best. Also, red is better than gray for all tasks. 
        </h2>


        <h2 class="title is-5">3. Can specialist models solve MUIRBench tasks? </h2>
        <h2 class="content has-text-justified">
          Specialists can serve as a proxy upper bound of how good multimodal LLMs could be. This sheds light on the possibility that multimodal LLMs may progress on these tasks given the correct data and training strategy. 
        </h2>
        <img src="static/images/specialist.png" height="100%"/>
        <h2 class="content has-text-justified">
          The specialists perform much better than GPT-4V and Gemini Pro, outperforming multimodal LLMs by 18% to 57% on these tasks. 
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Image carousel examples in MUIRBench-->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">MUIRBench Examples with GPT-4V Outputs</h2> <br></div>
      <div class="columns is-centered">
        <div class="column is-five-sixths">
      <h2 class="content has-text-justified">
        We show random selected actual-sized example data for the 14 tasks from <b>MUIRBench</b>, with GPT-4V predictions attached.
      </h2>
    </div></div>
  </div></section>
<section class="hero is-small">
  <div class="hero-body">
        <div class="container">
          <div id="results-carousel" class="carousel results-carousel">
           <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide1.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide2.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide3.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide4.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide5.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide6.png" width="100%"/>
            <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide7.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide8.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide9.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide10.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          <div class="item">
            <!-- Your image here -->
            <img src="static/images/example/Slide11.png" width="100%"/>
                        <!-- <h2 class="subtitle has-text-centered">
              Example case from <b>MUIRBench</b> and GPT-4V prediction.
            </h2> -->
          </div>
    
          </div>
        </div>
  </div>
</section>
<!-- End image carousel -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{wang2024muirbench,
          title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
          author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and others},
          journal={arXiv preprint arXiv:2406.09411},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
