<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="<b>MuirBench</b>: A Comprehensive Benchmark for
Robust Multi-image Understanding">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Benchmark, Multimodal Learning, Vision and Language Dataset, Vision Language Model, LLM, VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MuirBench</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://zeyofu.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://zeyofu.github.io/blink/">
            BLINK
          </a>
          <a class="navbar-item" href="https://zeyofu.github.io/CommonsenseT2I/">
            Commonsense-T2I
          </a>
        </div>
      </div>
    </div>

  </div>
  </nav>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://yushi-hu.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://finegrainedrlhf.github.io/">
            Fine-Grained RLHF (NeurIPS 2023)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><img src="static/images/icon.png" width="50" /> MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://feiwang96.github.io/" target="_blank"><font color="#B082C9"><b>Fei Wang</b></font></a><sup>1*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://zeyofu.github.io/" target="_blank"><font color="#B082C9"><b>Xingyu Fu</b></font></a><sup>2*</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://jyhuang36.github.io/" target="_blank"><font color="#B082C9"><b>James Y.Huang</b></font></a><sup>1†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://zekun-li.github.io/" target="_blank"><font color="#B082C9"><b>Zekun Li</b></font></a><sup>3†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://qinliu9.github.io/" target="_blank"><font color="#B082C9"><b>Qin Liu</b></font></a><sup>4†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://sheltonliu-n.github.io/" target="_blank"><font color="#B082C9"><b>Xiaogeng Liu</b></font></a><sup>5†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://derek.ma/" target="_blank"><font color="#B082C9"><b>Mingyu Derek Ma</b></font></a><sup>6†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://sites.google.com/site/xunannancy/" target="_blank"><font color="#B082C9"><b>Nan Xu</b></font></a><sup>1†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://wzhouad.github.io/" target="_blank"><font color="#B082C9"><b>Wenxuan Zhou</b></font></a><sup>1†</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://drogozhang.github.io/" target="_blank"><font color="#B082C9"><b>Kai Zhang</b></font></a><sup>7</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://tianyi-lorena-yan-me.web.app/" target="_blank"><font color="#B082C9"><b>Tianyi Lorena Yan</b></font></a><sup>1</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://jacky-laznake-personalweb.vercel.app/" target="_blank"><font color="#B082C9"><b>Wenjie Jacky Mo</b></font></a><sup>1</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://openreview.net/profile?id=~Hsiang-Hui_Liu1" target="_blank"><font color="#B082C9"><b>Hsiang-Hui Liu</b></font></a><sup>3</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://lupantech.github.io/" target="_blank"><font color="#B082C9"><b>Pan Lu</b></font></a><sup>6</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://chunyuan.li/" target="_blank"><font color="#B082C9"><b>Chunyuan Li</b></font></a><sup>8</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://xiaocw11.github.io/" target="_blank"><font color="#B082C9"><b>Chaowei Xiao</b></font></a><sup>5</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank"><font color="#B082C9"><b>Kai-Wei Chang</b></font></a><sup>6</sup>&emsp;
                </span>
                <br>
                <span class="author-block">
                  <a href="https://www.cis.upenn.edu/~danroth/" target="_blank"><font color="#B082C9"><b>Dan Roth</b></font></a><sup>2</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://sheng-z.github.io/" target="_blank"><font color="#B082C9"><b>Sheng Zhang</b></font></a><sup>9</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://www.microsoft.com/en-us/research/people/hoifung/" target="_blank"><font color="#B082C9"><b>Hoifunng Poon</b></font></a><sup>9</sup>&emsp;
                </span>
                <span class="author-block">
                  <a href="https://muhaochen.github.io/" target="_blank"><font color="#B082C9"><b>Muhao Chen</b></font></a><sup>4</sup>&emsp;
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>USC&emsp;
                      <sup>2</sup>UPenn&emsp;
                      <sup>3</sup>UMN&emsp;
                      <sup>4</sup>UC Davis&emsp;
                      <sup>5</sup>UW-Madison&emsp;
                      <sup>6</sup>UCLA&emsp;
                      <sup>7</sup>OSU&emsp;
                      <sup>8</sup>Bytedance&emsp;
                      <sup>9</sup>Microsoft Research&emsp;
                      <sup>*</sup>Equal Leadership&emsp;
                      <sup>†</sup>Equal Contribution
                    </span>
                    <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                  </div>

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.09411.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>HF Dataset</span>
                    </a>
                  </span>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/zeyofu/MuirBench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>

                <span class="link-block">
                  <a href="https://twitter.com/XingyuFu2/status/1781368539213082683" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
                </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3 has-text-centered">What is MuirBench?</h2>
      <h2 class="subtitle has-text-justified">
        <span style="font-weight:bold;">MuirBench  </span>
        is a benchmark containing 11,264 images and 2,600 multiple-choice questions, providing robust evaluation on 12 multi-image understanding tasks. </h2>
      <img src="static/images/teaser.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      <h2 class="hero-body has-text-centered">
        <!-- <br> -->
        Each example comes from one task in <span style="font-weight:bold;">MuirBench</span>, presenting diverse multi-image relations.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- MuirBench Comparison -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">MuirBench -- Novel Features</h2>
        <!-- <h2 class="title is-3">MuirBench Benchmark -- Unique Features of MuirBench?</h2> -->
        <h2 class="content has-text-justified">
          <img src="static/images/comparison.png" height="100%"/>
          <br>
        <ul>
          <li><b>MuirBench</b> evaluates on a <i><b>comprehensive range of 12 multi-image understanding abilities</b></i>, e.g. geographic understanding, diagram understanding, visual retrieval, ..., etc, while prior benchmarks generally contain single-image questions.</li>
          <li><b>MuirBench</b> contains <i><b>10 diverse multi-image relations</b></i>, e.g. narrative, complementary, etc. </li>
          <br>
        </ul>
        <div class="myrow">
          <div class="mycolumn">
            <img src="static/images/distribution.png" style="width:85%">
          </div>
          <div class="mycolumn">
            <img src="static/images/relation.png" style="width:100%">
          </div>
        </div>

        <h2 class="content has-text-justified">
        <ul>
          <br>
          <li><b>MuirBench</b> provides a <i><b>robust evaluation</b></i> on models by unanswerable instance variants. Three major ways to create the unanswerable instances are as below.</li>
          <br>
          <img src="static/images/unanswerable.png" height="100%"/>
        </ul>
        </h2>
        
        
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce <b>MuirBench</b>, a comprehensive benchmark that focuses on robust multi-image understanding capabilities of multimodal LLMs. <b>MuirBench</b> consists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that involve 10 categories of multi-image relations (e.g., multiview, temporal relations). Comprising 11,264 images and 2,600 multiple-choice questions, <b>MuirBench</b> is created in a pairwise manner, where each standard instance is paired with an unanswerable variant that has minimal semantic differences, in order for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our results reveal that even the best-performing models like GPT-4o and Gemini Pro find it challenging to solve <b>MuirBench</b>, achieving 68.0% and 49.3% in accuracy. Open-source multimodal LLMs trained on single images can hardly generalize to multi-image questions, hovering below 33.3% in accuracy. These results highlight the importance of <b>MuirBench</b> in encouraging the community to develop multimodal LLMs that can look beyond a single image, suggesting potential pathways for future improvements.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Results</h2>
        <img src="static/images/qual.png" width="100%"/>
        <!-- <h2 class="content has-text-centered">
          Qualitative results on <b>MuirBench</b>. 
        </h2> -->
        <h2 class="content has-text-justified">
          For each task, we show the ground truth (in blue), and choice of GPT-4o, Gemini Pro, and Mantis. Notice that the markers are intentionally added for visualization purposes.
        </h2>
        
      </div>
    </div>
  </div>
</section>


<!-- Paper Quantitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Quantitative Results</h2>
        <img src="static/images/quan.png" height="90%"/>
        <h2 class="content has-text-centered">
          Results of different models on <b>MuirBench</b>. The first row shows task names and number of test instances.  We see that most models perform similarly to random choice, and are far from humans.
        </h2>
        <h2 class="content has-text-justified">
          <!-- The mean accuracy of 7B and 13B open-source Multimodal LLMs hover around <b>35–42%</b>, which is similar to <b>random guess (38.09%)</b>. The most proficient open-source model, LLaVA-v1.6-34B, achieves an accuracy of 45.05%. Even the most advanced models, GPT-4V and Gemini Pro and Claude 3 OPUS, achieve accuracies of only 51.26%, 45.72%, and 44.11% respectively. Their performance are merely 13.17%, 7.63% and 6.02% better than random guessing and lag behind human performance by 44.44%, 49.98% and 51.59%. Notably, for certain tasks such as jigsaw, semantic correspondence, multi-view reasoning, object localization, and relative reflectance, some multimodal LLMs even underperform compared to random guessing. -->

          Overall performance: the average accuracies of the most advanced multimodal LLMs on <b>MuirBench</b> are no better than 68%, which are still far from enabling satisfactory utility. The mean accuracies of open-source multimodal LLMs that have considered multi-images hover between 23.73% and 44.50%, which fall behind from advanced proprietary LLMs. Notably, there is no obvious correlation between model sizes and performances, indicating the importance of training data and training processes in developing multimodal LLMs with multi-image understanding capabilities. For certain models and tasks, some results are only on par or even below random guessing.
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper Analysis -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Experiment Analysis</h2> <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">
        1. In which multi-image tasks do multimodal LLMs show relative strengths and weaknesses?
        <h2 class="content has-text-justified">
          As in the figure, we observe that multimodal LLMs perform relatively better on image-text matching, visual retrieval, and diagram understanding. In contrast, multi-image ordering and visual grounding appear to be more challenging for these models, because these tasks require understanding the whole multi-image context and conducting more complicated reasoning processes across images and modalities afterwards.
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/radar_v1.png" width="60%"/> </div>
        <br>
        <h2 class="title is-5">2. Do multimodal LLMs perform worse on the unanswerable set?</h2>
        <h2 class="content has-text-justified">
          We compare performances on answerable and unanswerable sets for some best-performing models. All the studied models have <b>severe performance drop</b> when changing answerable instances to unanswerable counterparts. A closer look of the error cases reveals that models often avoid abstention when facing unanswerable questions. These observations not only highlight the importance of assessing model behavior under a more realistic setting, but also show that the pairwise design improves the reliability of <b>MuirBench</b>.
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/unanswerable_perf.png" width="60%"/> </div>
        <br>
        <h2 class="title is-5">3. Are errors caused by specific image positions or unanserable types? </h2>
        <h2 class="content has-text-justified">
          <ul>
            <li>
              As in the left figure, we analyze the error rates of varying input positions of images and report the performance of GPT-4o, GeminiProVision, and Mantis-8B-Idefics2. The highest accuracy is achieved when images are positioned in options, while the highest error rate can be observed when images are in the middle of questions. This consistent trend across different models suggests that the position of images within a question correlates with the error rate.
            </li>
            <li>
              As in the right figure, results show that the error rate also correlates with the type of unanswerable instances. All the three models perform relatively better when we only change the questions to make it incompatible with original images and options. However, all models are confused when the correct option is removed and fail to choose “none of the other options” in this scenario. The performance on unanswerable instances created by reordering or replacing images is divergent. Notably, GPT-4o performs much better than the other models in these cases.
            </li>
          </ul>          
        </h2>

        <div class="myrow">
          <div class="mycolumn">
            <img src="static/images/perf_image_position.png" style="width:95%">
          </div>
          <div class="mycolumn">
            <img src="static/images/perf_no_answer.png" style="width:95%">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @article{wang2024muirbench,
          title={MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding},
          author={Wang, Fei and Fu, Xingyu and Huang, James Y and Li, Zekun and Liu, Qin and Liu, Xiaogeng and Ma, Mingyu Derek and Xu, Nan and Zhou, Wenxuan and Zhang, Kai and others},
          journal={arXiv preprint arXiv:2406.09411},
          year={2024}
        }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
